{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim import utils\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Add, Multiply\n",
    "from keras.models import Model, Sequential\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills = pd.read_csv('data/bill_all.csv')\n",
    "print(df_bills.columns)\n",
    "df_bills.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votes = pd.read_csv('data/votes_all.csv')\n",
    "print(df_votes.columns)\n",
    "df_votes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('data/df_vote_final.csv')\n",
    "df_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_votes = df_votes[df_votes['legis_num'] != 'ADJOURN']\n",
    "\n",
    "\n",
    "\n",
    "def gen_x():\n",
    "    df_list = []\n",
    "    for name, group in tqdm(df_votes.groupby('legis_num')):\n",
    "    #     print('Bill', name)\n",
    "    #     print('vote shape', group.shape)\n",
    "        bill = df_bills[df_bills['legis_num'] == name]\n",
    "        group.reset_index(inplace=True)\n",
    "    #     print( bill['sponsor'])\n",
    "        group.loc[:,'sponsor'] = bill.iloc[0]['sponsor']\n",
    "        group.loc[:,'sponsor_id'] = bill.iloc[0]['sponsor_id']\n",
    "        group.loc[:,'sponsor_party'] = bill.iloc[0]['sponsor_party']\n",
    "        group.loc[:,'sponsor_state'] = bill.iloc[0]['sponsor_state']\n",
    "        group.loc[:,'sponsor_uri'] = bill.iloc[0]['sponsor_uri']\n",
    "        df_list.append(group)\n",
    "    #     print(group.columns)\n",
    "    #     break\n",
    "    return df_list\n",
    "   \n",
    "# df_list = gen_x()\n",
    "    \n",
    "# df_final = pd.concat(df_list)\n",
    "\n",
    "df_final.reset_index(inplace=True)\n",
    "df_final.to_csv('data/df_vote_final.csv')\n",
    "df_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[df_final['sponsor_party'].isin(['D', 'R'])]\n",
    "print((df_final['sponsor'].value_counts()[:10]))\n",
    "df_final['sponsor'].value_counts()[:10].plot(kind='bar', alpha=.5)\n",
    "plt.show()\n",
    "df_final['sponsor_party'].value_counts()[:10].plot(kind='bar', alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/sonic/.keras/datasets/GoogleNews-vectors-negative300.bin',\n",
    "                                                        binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_words = 20000\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "def process_doc(X):\n",
    "    tokenizer = Tokenizer(num_words=max_words,lower=True, split=' ', \n",
    "                          filters='\"#%&()*+-/<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                          char_level=False, oov_token=u'<UNK>')\n",
    "\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    X_seq = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X)\n",
    "\n",
    "    tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "    X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "    x_emb =[]\n",
    "    for doc in X: #look up each doc in model\n",
    "        x_emb.append(document_vector(model, doc))\n",
    "\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "#     X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH,  padding=\"post\", truncating=\"post\")\n",
    "    return np.array(X_seq), word_index, np.array(x_emb), X_train_tf, X_train_counts\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
    "    return np.mean(word2vec_model[doc], axis=0)\n",
    "\n",
    "\n",
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.vocab for word in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "AYE = 1\n",
    "NAY = -1\n",
    "\n",
    "vote_matrix_all = {}\n",
    "X_seq_all = {}\n",
    "word_index_all = {}\n",
    "X_train_tf_all = {}\n",
    "X_train_counts_all = {}\n",
    "X_emb_all = {}\n",
    "legistlator_dict_all = {}\n",
    "feature_set_all = {}\n",
    "\n",
    "\n",
    "grouped_congress = df_bills.groupby('congress')\n",
    "\n",
    "\n",
    "for name, group in grouped_congress:\n",
    "    print('Processing congress', name)\n",
    "    print('congress shape', group.shape)\n",
    "    9\n",
    "    df_votes_filtered = df_final[df_final['congress'] == name]\n",
    "    \n",
    "    # Feature set \n",
    "    feature_set = df_votes_filtered[['sponsor_id', 'sponsor_party', 'sponsor_state', 'vote']]\n",
    "    \n",
    "    num_legistlators = len(df_votes_filtered['name'].unique())\n",
    "    print('number of legistlators', num_legistlators)\n",
    "    vote_matrix = np.zeros((group.shape[0], num_legistlators))\n",
    "    print('vote_matrix.shape', vote_matrix.shape)\n",
    "    \n",
    "    # Creat legistor dict mapping!\n",
    "    legistlator_dict = {}\n",
    "    i = 0\n",
    "    for legitslator in df_votes_filtered['name'].unique():\n",
    "        legistlator_dict[legitslator] = i\n",
    "        i += 1\n",
    "    \n",
    "    legistlator_dict_all[name] = {v: k for k, v in legistlator_dict.items()} \n",
    "    \n",
    "#     print(legistlator_dict)\n",
    "    print('Processing congress votes')\n",
    "    group.reset_index(inplace=True)\n",
    "    \n",
    "    vote_feature = []\n",
    "    # Process Vote matrix here\n",
    "    for index, row in tqdm(group.iterrows()):\n",
    "        \n",
    "        df_filtered = df_votes_filtered[df_votes_filtered['legis_num'] == row['legis_num']]\n",
    "\n",
    "        for _, vote in df_filtered.iterrows():\n",
    "            if vote['vote'] == 'Yea':\n",
    "                vote_matrix[index, legistlator_dict[vote['name']]] = AYE\n",
    "            elif vote['vote'] == 'Nay':\n",
    "                vote_matrix[index, legistlator_dict[vote['name']]] = NAY\n",
    "\n",
    "    # Process bill Representation Here\n",
    "    print('processing congress bills')\n",
    "    X_seq, word_index, x_emb, X_train_tf, X_train_counts = process_doc(group['billText'].apply(str))\n",
    "\n",
    "    vote_matrix_all[name] = vote_matrix\n",
    "    X_seq_all[name] = X_seq\n",
    "    word_index_all[name] = word_index\n",
    "    X_emb_all[name] = x_emb\n",
    "    X_train_tf_all[name] = X_train_tf\n",
    "    X_train_counts_all[name] = X_train_counts\n",
    "    feature_set_all[name] = feature_set\n",
    "    \n",
    "    print('*' * 50)\n",
    "#     break\n",
    "    \n",
    "# print('feature_all', feature_all[106].shape)\n",
    "# print('y_all', y_all[106].shape)\n",
    "   \n",
    "print('vote_matrix_all.shape', vote_matrix_all[106].shape)\n",
    "np.save('data/vote_matrix_all.npy', vote_matrix_all)\n",
    "np.save('data/X_seq_all.npy', X_seq_all)\n",
    "np.save('data/X_word_index_all.npy', word_index_all)\n",
    "np.save('data/X_train_tf_all.npy', X_train_tf_all)\n",
    "np.save('data/X_train_counts_all.npy', X_train_counts_all)\n",
    "np.save('data/X_emb_all.npy', X_emb_all)\n",
    "np.save('data/legistlator_all.npy', legistlator_dict_all)\n",
    "np.save('data/feature_set_all.npy', feature_set_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vote_matrix_all = np.load('data/vote_matrix_all.npy' )\n",
    "X_seq_all = np.load('data/X_seq_all.npy')\n",
    "word_index_all = np.load('data/X_word_index_all.npy')\n",
    "X_train_tf_all = np.load('data/X_train_tf_all.npy')\n",
    "X_train_counts_all = np.load('data/X_train_counts_all.npy')\n",
    "X_emb_all = np.load('data/X_emb_all.npy')\n",
    "legistlator_all = np.load('data/legistlator_all.npy')\n",
    "feature_set_all = np.load('data/feature_set_all.npy')\n",
    "\n",
    "print('vote_matrix_all',vote_matrix_all.item()[106].shape)\n",
    "print('X_seq_all', X_seq_all.item()[106].shape)\n",
    "# print(word_index_all[106].shape)\n",
    "print('X_train_tf_all', X_train_tf_all.item()[106].shape)\n",
    "print('X_train_counts_all', X_train_counts_all.item()[106].shape)\n",
    "print('X_emb_all', X_emb_all.item()[106].shape)\n",
    "# print('legistlator_all', legistlator_all.item()[106].shape)\n",
    "print('feature_set_all', feature_set_all.item()[106].shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variational\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
    "\n",
    "    # keras.losses.binary_crossentropy gives the mean\n",
    "    # over the last axis. we require the sum\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "\n",
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def get_VAE(original_dim):\n",
    "    decoder = Sequential([\n",
    "        Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "        Dense(original_dim, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    x = Input(shape=(original_dim,))\n",
    "    h = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "    z_mu = Dense(latent_dim)(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "    z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "    z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "    eps = Input(tensor=K.random_normal(stddev=epsilon_std,\n",
    "                                       shape=(K.shape(x)[0], latent_dim)))\n",
    "    z_eps = Multiply()([z_sigma, eps])\n",
    "    z = Add()([z_mu, z_eps])\n",
    "\n",
    "    x_pred = decoder(z)\n",
    "\n",
    "    vae = Model(inputs=[x, eps], outputs=x_pred)\n",
    "    \n",
    "    loss = nll\n",
    "    loss = 'mean_squared_error'\n",
    "    vae.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "    encoder = Model(x, z_mu)\n",
    "    return vae, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import glorot_uniform  # Or your initializer of choice\n",
    "\n",
    "def reinitialize(model):\n",
    "    initial_weights = model.get_weights()\n",
    "    new_weights = [glorot_uniform()(w.shape).eval() for w in initial_weights]\n",
    "    model.set_weights(new_weights)\n",
    "    return model\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb = X_emb_all.item()[106]\n",
    "vote_matrix = vote_matrix_all.item()[106]\n",
    "print('X_emb', X_emb.shape)\n",
    "print('vote_matrix', vote_matrix.shape)\n",
    "\n",
    "# numpyMatrix = df.as_matrix().astype(float)\n",
    "# scaled_data = preprocessing.scale(numpyMatrix)\n",
    "\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, StandardScaler\n",
    "# X_emb = StandardScaler().fit_transform(X_emb.astype(float))\n",
    "X_emb = scale(X_emb.astype(float))\n",
    "\n",
    "X = []\n",
    "X_meta = []\n",
    "y = []\n",
    "i = 0\n",
    "\n",
    "#     mean = 0.0   # some constant\n",
    "#     std = 1.0    # some constant (standard deviation)\n",
    "#     meta = meta + np.random.normal(mean, std, meta.shape)\n",
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "noise_factor = 0.5\n",
    "\n",
    "X_train = []\n",
    "######\n",
    "# Create Meta for each legistlator\n",
    "for idx, legistlator in enumerate(vote_matrix.T):\n",
    "#     print('np.vstack(legistlator)', np.vstack(legistlator).shape)\n",
    "#     print('legistlator.shape', legistlator.shape)\n",
    "#     legistlator = legistlator + np.random.normal(mu, sigma, legistlator.shape)\n",
    "\n",
    "    meta = np.multiply(X_emb, np.vstack(legistlator)) # Eelementwise multiplication, introducing noise\n",
    "\n",
    "\n",
    "    meta = meta + noise_factor * np.random.normal(mu, sigma, meta.shape)\n",
    "\n",
    "#     print('meta.shape', meta.shape)\n",
    "    \n",
    "    X_meta.append(meta)\n",
    "    X_train.append(X_emb)\n",
    "\n",
    "#     break\n",
    "######\n",
    "X_meta = np.array(X_meta)\n",
    "X_train = np.array(X_train)\n",
    "print('X_meta', X_meta.shape)\n",
    "print('X_train', X_train.shape)\n",
    "\n",
    "\n",
    "# Reshape to flatten the dimentions\n",
    "# X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "# X_meta = X_meta.reshape(X_meta.shape[0], -1)\n",
    "# X_train = X_train.reshape(-1, X_train.shape[1], X_train.shape[2], 1)\n",
    "# X_meta = X_meta.reshape(-1, X_meta.shape[1], X_meta.shape[2], 1)\n",
    "\n",
    "X_train =  np.clip(X_train, -1., 1.)\n",
    "X_meta = np.clip(X_meta, -1., 1.)\n",
    "print('X_train new shape', X_train.shape)\n",
    "print('X_meta new shape', X_meta.shape)\n",
    "\n",
    "print(X_train[0].shape)\n",
    "print(X_meta[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_autoencoder(X_train):\n",
    "    input_img = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    encoded = Dense(128, activation='relu', kernel_initializer='glorot_uniform')(input_img)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu',  name='encoded')(encoded)\n",
    "\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(X_train.shape[2], activation='sigmoid')(decoded)\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "#     loss = 'mean_squared_error'\n",
    "    loss='binary_crossentropy'\n",
    "    autoencoder.compile(optimizer='adam', loss=loss)\n",
    "    return autoencoder\n",
    "\n",
    "def denoiser_autoencoder(X_train):\n",
    "#     input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "    input_img = Input(shape = (X_train.shape[1], X_train.shape[2], 1 ))\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='glorot_uniform')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # at this point the representation is (7, 7, 32)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return autoencoder\n",
    "\n",
    "    \n",
    "from keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D\n",
    "\n",
    "\n",
    "def conv_autoencoder(X_train):\n",
    "    \n",
    "    input_img = Input(shape = (1, X_train.shape[1], X_train.shape[2] ))\n",
    "    #encoder\n",
    "    #input = 28 x 28 x 1 (wide and thin)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', name='encoded')(pool2) #7 x 7 x 128 (small and thick)\n",
    "\n",
    "    #decoder\n",
    "    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 128\n",
    "    up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n",
    "    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64\n",
    "    up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='decoded')(up2) # 28 x 28 x 1\n",
    "    \n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(loss='mean_squared_error', optimizer = 'RMSprop')\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# original_dim = \n",
    "intermediate_dim = 256\n",
    "latent_dim = 128\n",
    "batch_size = 256\n",
    "epochs = 20\n",
    "epsilon_std = 1.0\n",
    "###################\n",
    "\n",
    "# autoencoder, encoder, decoder = get_VAE(original_dim)\n",
    "autoencoder = deep_autoencoder(X_train)\n",
    "# autoencoder = denoiser_autoencoder(X_train)\n",
    "# autoencoder = conv_autoencoder(X_train)\n",
    "print(autoencoder.summary())\n",
    "\n",
    "rs = ShuffleSplit(n_splits=3, test_size=.25, random_state=0)\n",
    "rs.get_n_splits(X_train)\n",
    "\n",
    "print(rs)\n",
    "\n",
    "def plot_history(history):\n",
    "    print(history.history)\n",
    "    df = pd.DataFrame(history.history)\n",
    "#     print(df.tail())\n",
    "    df.plot(xticks=range(epochs))\n",
    "#     print(history.history.keys())\n",
    "\n",
    "    \n",
    "for train_index, test_index in rs.split(X_train):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_emb_train, X_emb_test = X_train[train_index], X_train[test_index]\n",
    "    X_meta_train, X_meta_test = X_meta[train_index], X_meta[test_index]\n",
    "    \n",
    "    print(X_emb_train.shape, X_emb_test.shape)\n",
    "    print(X_meta_train.shape, X_meta_test.shape)\n",
    "#     break\n",
    "    \n",
    "    history = autoencoder.fit(X_emb_train,\n",
    "        X_meta_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    plot_history(history)\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    names = [weight.name for layer in autoencoder.layers for weight in layer.weights]\n",
    "    weights = autoencoder.get_weights()\n",
    "\n",
    "    for name, weight in zip(names, weights):\n",
    "        print(name, weight.shape)\n",
    "        \n",
    "#     encoded_weight = \n",
    "#     print(model_weights['encoded'].shape, model_weights['encoded'])\n",
    "\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_set = feature_set_all.item()[106]\n",
    "\n",
    "X = feature_set[['sponsor_id', 'sponsor_party', 'sponsor_state']]\n",
    "y = feature_set['vote']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "#     print(le.classes_)\n",
    "y = le.transform(y)\n",
    "    \n",
    "# split into a training and testing set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
