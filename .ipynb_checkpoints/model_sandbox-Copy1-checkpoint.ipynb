{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim import utils\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "tqdm.pandas(desc='progress-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills = pd.read_csv('data/bill_all.csv')\n",
    "print(df_bills.columns)\n",
    "df_bills.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_bills['sponsor'].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_votes = pd.read_csv('data/votes_all.csv')\n",
    "print(df_votes.columns)\n",
    "df_votes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votes.tail()\n",
    "# print(df_votes[1])\n",
    "\n",
    "df_votes = df_votes[df_votes['legis_num'] != 'ADJOURN']\n",
    "\n",
    "df_list = []\n",
    "for name, group in tqdm(df_votes.groupby('legis_num')):\n",
    "#     print('Bill', name)\n",
    "#     print('vote shape', group.shape)\n",
    "    bill = df_bills[df_bills['legis_num'] == name]\n",
    "    group.reset_index(inplace=True)\n",
    "#     print( bill['sponsor'])\n",
    "    group['sponsor'] = bill.iloc[0]['sponsor']\n",
    "    group['sponsor_id'] = bill.iloc[0]['sponsor_id']\n",
    "    group['sponsor_party'] = bill.iloc[0]['sponsor_party']\n",
    "    group['sponsor_state'] = bill.iloc[0]['sponsor_state']\n",
    "    group['sponsor_uri'] = bill.iloc[0]['sponsor_uri']\n",
    "    df_list.append(group)\n",
    "#     print(group.columns)\n",
    "#     break\n",
    "    \n",
    "df_final = pd.concat(df_list)\n",
    "\n",
    "df_final.reset_index(inplace=True)\n",
    "df_final.to_csv('data/df_vote_final.csv')\n",
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('data/df_vote_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['sponsor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.dropna(subset=['sponsor'])\n",
    "df_final['vote_desc'].replace(np.nan, '', regex=True)\n",
    "# df_final.reset_index(inplace=True)\n",
    "df_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer, FeatureHasher\n",
    "encoder = FeatureHasher(n_features=10, input_type=\"string\")\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "feature_all = {}\n",
    "y_all = {}\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "n_components = 20\n",
    "    \n",
    "for name, group in tqdm(df_final.groupby('congress')):\n",
    "    print('Processing congress', name)\n",
    "    print('congress shape', group.shape)  \n",
    "    \n",
    "#     print(encoder.fit_transform(group[['sponsor_id', 'sponsor_party', 'sponsor_state']]).shape)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=10000,\n",
    "                                   stop_words='english')\n",
    "\n",
    "    \n",
    "    group['sponsor_id'] = encoder.fit_transform(group['sponsor_id'])\n",
    "    group['sponsor_party'] = encoder.fit_transform(group['sponsor_party'])\n",
    "    group['sponsor_state'] = encoder.fit_transform(group['sponsor_state'])\n",
    "#     tf_idf_desc = tfidf_vectorizer.fit_transform(group['vote_desc'].values.astype('U'))\n",
    "#     print('tf_idf shape', tf_idf_desc.shape)\n",
    "#     nmf = NMF(n_components=n_components, \n",
    "#               random_state=1, beta_loss='kullback-leibler', \n",
    "#               solver='mu', max_iter=1000, alpha=.1, l1_ratio=.5).fit_transform(tf_idf_desc)\n",
    "#     print('nmf shape', nmf.shape)\n",
    "\n",
    "    X = group[['sponsor_id', 'sponsor_party', 'sponsor_state']]\n",
    "#     print(X)\n",
    "#     X = np.hstack((group['sponsor_id'].values.reshape(-1,1), \n",
    "#                    group['sponsor_party'].values.reshape(-1,1), \n",
    "#                    group['sponsor_state'].values.reshape(-1,1)))\n",
    "    \n",
    "\n",
    "#     X = np.hstack((encoder.fit_transform(group['sponsor_id']).reshape(-1,1), \n",
    "#                    encoder.fit_transform(group['sponsor_party']).reshape(-1,1), \n",
    "#                    encoder.fit_transform(group['sponsor_state']).reshape(-1,1)))\n",
    "#     X = pd.DataFrame(X)\n",
    "#     print(X.describe())\n",
    "#     print(list(encoder.classes_))\n",
    "\n",
    "    y = group['vote']\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y)\n",
    "#     print(le.classes_)\n",
    "    y = le.transform(y)\n",
    "    print(X[:1])\n",
    "    print(y[:1])\n",
    "\n",
    "    print('X shape', X.shape)\n",
    "    print('y shape', y.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=42)\n",
    "\n",
    "    print('X_train', X_train.shape, 'y_train', y_train.shape)\n",
    "    print('X_test', X_test.shape, 'y_test', y_test.shape)\n",
    "    \n",
    "    clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "#     clf = SVC(kernel=\"linear\", C=0.025)\n",
    "#     clf = MLPClassifier(alpha=1)\n",
    "#     clf = AdaBoostClassifier()\n",
    "#     clf = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "#     clf = DecisionTreeClassifier(max_depth=5)\n",
    "    clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "#     clf = KNeighborsClassifier(3)\n",
    "#     clf = GaussianNB()\n",
    "    print(clf)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_predict = clf.predict(X_test)\n",
    "\n",
    "    regression_model_mse = mean_squared_error(y_test, y_predict)\n",
    "    print('model_mse', regression_model_mse)\n",
    "    print('accuracy', accuracy_score(y_test, y_predict))\n",
    "\n",
    "    print(confusion_matrix(y_test, y_predict))\n",
    "    print(classification_report(y_test, y_predict, target_names=le.classes_))\n",
    "    \n",
    "    print((group['vote'].value_counts()))\n",
    "#     group['vote'].value_counts().plot(kind='bar', alpha=.5)\n",
    "    group['sponsor_state'].value_counts()[:10].plot(kind='bar', alpha=.5)\n",
    "             \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_bills['congress'].value_counts()))\n",
    "df_bills['congress'].value_counts().plot(kind='bar', alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_votes['vote'].value_counts()))\n",
    "df_votes['vote'].value_counts().plot(kind='bar', alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_votes['vote_result'].value_counts()))\n",
    "df_votes['vote_result'].value_counts().plot(kind='bar', alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat Vote Matrix and Bill representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/sonic/.keras/datasets/GoogleNews-vectors-negative300.bin',\n",
    "                                                        binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "max_words = 20000\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "def process_doc(X):\n",
    "    tokenizer = Tokenizer(num_words=max_words,lower=True, split=' ', \n",
    "                          filters='\"#%&()*+-/<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                          char_level=False, oov_token=u'<UNK>')\n",
    "\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    X_seq = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X)\n",
    "\n",
    "    tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "    X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "    x_emb =[]\n",
    "    for doc in X: #look up each doc in model\n",
    "        x_emb.append(document_vector(model, doc))\n",
    "\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "#     X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH,  padding=\"post\", truncating=\"post\")\n",
    "    return np.array(X_seq), word_index, np.array(x_emb), X_train_tf, X_train_counts\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
    "    return np.mean(word2vec_model[doc], axis=0)\n",
    "\n",
    "\n",
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.vocab for word in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "AYE = 1\n",
    "NAY = -1\n",
    "\n",
    "vote_matrix_all = {}\n",
    "X_seq_all = {}\n",
    "word_index_all = {}\n",
    "X_train_tf_all = {}\n",
    "X_train_counts_all = {}\n",
    "X_emb_all = {}\n",
    "legistlator_dict_all = {}\n",
    "\n",
    "\n",
    "grouped_congress = df_bills.groupby('congress')\n",
    "\n",
    "\n",
    "for name, group in grouped_congress:\n",
    "    print('Processing congress', name)\n",
    "    print('congress shape', group.shape)\n",
    "    \n",
    "    df_votes_filtered = df_votes[df_votes['congress'] == name]\n",
    "    \n",
    "    num_legistlators = len(df_votes_filtered['name'].unique())\n",
    "    print('number of legistlators', num_legistlators)\n",
    "    vote_matrix = np.zeros((group.shape[0], num_legistlators))\n",
    "    print('vote_matrix.shape', vote_matrix.shape)\n",
    "    \n",
    "    # Creat legistor dict mapping!\n",
    "    legistlator_dict = {}\n",
    "    i = 0\n",
    "    for legitslator in df_votes_filtered['name'].unique():\n",
    "        legistlator_dict[legitslator] = i\n",
    "        i += 1\n",
    "    \n",
    "    legistlator_dict_all[name] = {v: k for k, v in legistlator_dict.items()} \n",
    "    \n",
    "#     print(legistlator_dict)\n",
    "    print('Processing congress votes')\n",
    "    group.reset_index(inplace=True)\n",
    "    \n",
    "    vote_feature = []\n",
    "    # Process Vote matrix here\n",
    "    for index, row in tqdm(group.iterrows()):\n",
    "        \n",
    "        df_filtered = df_votes_filtered[df_votes_filtered['legis_num'] == row['legis_num']]\n",
    "\n",
    "        for _, vote in df_filtered.iterrows():\n",
    "            if vote['vote'] == 'Yea':\n",
    "                vote_matrix[index, legistlator_dict[vote['name']]] = AYE\n",
    "            elif vote['vote'] == 'Nay':\n",
    "                vote_matrix[index, legistlator_dict[vote['name']]] = NAY\n",
    "\n",
    "    # Process bill Representation Here\n",
    "    print('processing congress bills')\n",
    "    X_seq, word_index, x_emb, X_train_tf, X_train_counts = process_doc(group['billText'].apply(str))\n",
    "\n",
    "    vote_matrix_all[name] = vote_matrix\n",
    "    X_seq_all[name] = X_seq\n",
    "    word_index_all[name] = word_index\n",
    "    X_emb_all[name] = x_emb\n",
    "    X_train_tf_all[name] = X_train_tf\n",
    "    X_train_counts_all[name] = X_train_counts\n",
    "    \n",
    "    print('*' * 50)\n",
    "#     break\n",
    "    \n",
    "# print('feature_all', feature_all[106].shape)\n",
    "# print('y_all', y_all[106].shape)\n",
    "   \n",
    "print('vote_matrix_all.shape', vote_matrix_all[106].shape)\n",
    "np.save('data/vote_matrix_all.npy', vote_matrix_all)\n",
    "np.save('data/X_seq_all.npy', X_seq_all)\n",
    "np.save('data/X_word_index_all.npy', word_index_all)\n",
    "np.save('data/X_train_tf_all.npy', X_train_tf_all)\n",
    "np.save('data/X_train_counts_all.npy', X_train_counts_all)\n",
    "np.save('data/X_emb_all.npy', X_emb_all)\n",
    "np.save('data/legistlator_all.npy', legistlator_dict_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vo_df = df_votes.concat(df_bills, on='legis_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_train = {}\n",
    "for name, group in df_votes.groupby('congress'):\n",
    "    print('Processing congress', name)\n",
    "    print('congress shape', group.shape)\n",
    "    vote\n",
    "    \n",
    "#     vote_train[name] = group[['majority', 'party', 'sponser', 'vote_result', 'vote', ]]\n",
    "    \n",
    "    break\n",
    "\n",
    "# vote_train[106].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vote_matrix_all[106].shape)\n",
    "print(X_seq_all[106].shape)\n",
    "# print(word_index_all[106].shape)\n",
    "print(X_train_tf_all[106].shape)\n",
    "print(X_train_counts_all[106].shape)\n",
    "print(X_emb_all[106].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = 115\n",
    "x = X_emb_all[congress]\n",
    "print('x', x.shape)\n",
    "df_congress = df_bills[df_bills['congress'] == congress]\n",
    "y = df_congress['party'].values\n",
    "print(y.shape)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "print(le.classes_)\n",
    "y = le.transform(y)\n",
    "print(y)\n",
    "\n",
    "colors = ['blue', 'red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty Useless :) \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x)\n",
    "\n",
    "plt.figure(1, figsize=(20, 20),)\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], s=100, c=y, cmap=matplotlib.colors.ListedColormap(colors), alpha=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X_tsne = TSNE(n_components=2, verbose=2).fit_transform(x)\n",
    "\n",
    "plt.figure(1, figsize=(20, 20),)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=100, c=y, cmap=matplotlib.colors.ListedColormap(colors), alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
