{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Flatten\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "from Dataset import Dataset\n",
    "from evaluate import evaluate_model\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import math\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_normal(shape, name=None):\n",
    "#     return initializers.RandomUniform(shape=shape, minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "def get_model(num_users, num_items, latent_dim, regs=[0,0]):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim,\n",
    "                                  name = 'user_embedding', init='random_uniform',\n",
    "                                  W_regularizer = l2(regs[0]), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, \n",
    "                                  name = 'item_embedding', init='random_uniform',\n",
    "                                  W_regularizer = l2(regs[1]), input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings \n",
    "    predict_vector = keras.layers.concatenate([user_latent, item_latent])\n",
    "    \n",
    "    # Final prediction layer\n",
    "    #prediction = Lambda(lambda x: K.sigmoid(K.sum(x)), output_shape=(1,))(predict_vector)\n",
    "    prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = 'prediction')(predict_vector)\n",
    "    \n",
    "    model = Model(input=[user_input, item_input], output=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        print(type(train))\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while train.has_key((u, j)):\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done [9.2 s]. #user=6040, #item=3706, #train=994169, #test=6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=6040, embeddings_regularizer=<keras.reg..., input_length=1, output_dim=8, name=\"user_embedding\", embeddings_initializer=\"random_uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=3706, embeddings_regularizer=<keras.reg..., input_length=1, output_dim=8, name=\"item_embedding\", embeddings_initializer=\"random_uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"lecun_uniform\", name=\"prediction\", activation=\"sigmoid\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"pr..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.0921, NDCG = 0.0420\t [2.5 s]\n",
      "<class 'scipy.sparse.dok.dok_matrix'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "has_key not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dcae87dbaa0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Generate training instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_negatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-967fcb0e3450>\u001b[0m in \u001b[0;36mget_train_instances\u001b[0;34m(train, num_negatives)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_negatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: has_key not found"
     ]
    }
   ],
   "source": [
    "#################### Arguments ####################\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run GMF.\")\n",
    "    parser.add_argument('--path', nargs='?', default='Data/',\n",
    "                        help='Input data path.')\n",
    "    parser.add_argument('--dataset', nargs='?', default='ml-1m',\n",
    "                        help='Choose a dataset.')\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='Number of epochs.')\n",
    "    parser.add_argument('--batch_size', type=int, default=256,\n",
    "                        help='Batch size.')\n",
    "    parser.add_argument('--num_factors', type=int, default=8,\n",
    "                        help='Embedding size.')\n",
    "    parser.add_argument('--regs', nargs='?', default='[0,0]',\n",
    "                        help=\"Regularization for user and item embeddings.\")\n",
    "    parser.add_argument('--num_neg', type=int, default=4,\n",
    "                        help='Number of negative instances to pair with a positive instance.')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--learner', nargs='?', default='adam',\n",
    "                        help='Specify an optimizer: adagrad, adam, rmsprop, sgd')\n",
    "    parser.add_argument('--verbose', type=int, default=1,\n",
    "                        help='Show performance per X iterations')\n",
    "    parser.add_argument('--out', type=int, default=1,\n",
    "                        help='Whether to save the trained model.')\n",
    "    return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "num_factors = 8\n",
    "regs = eval('[0,0]')\n",
    "num_negatives = 4 # args.num_neg\n",
    "learner = 'adam' # args.learner\n",
    "learning_rate = 0.001 #args.lr\n",
    "epochs = 100 # args.epochs\n",
    "batch_size = 256 # args.batch_size\n",
    "verbose = 1 # args.verbose\n",
    "dataset = 'ml-1m'\n",
    "path = 'Data/'\n",
    "\n",
    "topK = 10\n",
    "evaluation_threads = 1 #mp.cpu_count()\n",
    "# print(\"GMF arguments: %s\" %(args))\n",
    "model_out_file = 'Pretrain/%s_GMF_%d_%d.h5' %(dataset, num_factors, time())\n",
    "\n",
    "# Loading data\n",
    "t1 = time()\n",
    "dataset = Dataset(path + dataset)\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "num_users, num_items = train.shape\n",
    "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "      %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
    "\n",
    "# Build model\n",
    "model = get_model(num_users, num_items, num_factors, regs)\n",
    "if learner.lower() == \"adagrad\": \n",
    "    model.compile(optimizer=Adagrad(lr=learning_rate), loss='binary_crossentropy')\n",
    "elif learner.lower() == \"rmsprop\":\n",
    "    model.compile(optimizer=RMSprop(lr=learning_rate), loss='binary_crossentropy')\n",
    "elif learner.lower() == \"adam\":\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "else:\n",
    "    model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy')\n",
    "#print(model.summary())\n",
    "\n",
    "# Init performance\n",
    "t1 = time()\n",
    "(hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "#mf_embedding_norm = np.linalg.norm(model.get_layer('user_embedding').get_weights())+np.linalg.norm(model.get_layer('item_embedding').get_weights())\n",
    "#p_norm = np.linalg.norm(model.get_layer('prediction').get_weights()[0])\n",
    "print('Init: HR = %.4f, NDCG = %.4f\\t [%.1f s]' % (hr, ndcg, time()-t1))\n",
    "\n",
    "# Train model\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
    "for epoch in range(epochs):\n",
    "    t1 = time()\n",
    "    # Generate training instances\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "\n",
    "    # Training\n",
    "    hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                     np.array(labels), # labels \n",
    "                     batch_size=batch_size, nb_epoch=1, verbose=0, shuffle=True)\n",
    "    t2 = time()\n",
    "\n",
    "    # Evaluation\n",
    "    if epoch %verbose == 0:\n",
    "        (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "        hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "        print('Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' \n",
    "              % (epoch,  t2-t1, hr, ndcg, loss, time()-t2))\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "            if args.out > 0:\n",
    "                model.save_weights(model_out_file, overwrite=True)\n",
    "\n",
    "print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))\n",
    "if out > 0:\n",
    "    print(\"The best GMF model is saved to %s\" %(model_out_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
