{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Dec 9, 2015\n",
    "\n",
    "@author: donghyun\n",
    "'''\n",
    "import argparse\n",
    "import sys\n",
    "from data_manager import Data_Factory\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Option for pre-processing data\n",
    "parser.add_argument(\"-c\", \"--do_preprocess\", type=bool,\n",
    "                    help=\"True or False to preprocess raw data for ConvMF (default = False)\", default=False)\n",
    "parser.add_argument(\"-r\", \"--raw_rating_data_path\", type=str,\n",
    "                    help=\"Path to raw rating data. data format - user id::item id::rating\")\n",
    "parser.add_argument(\"-i\", \"--raw_item_document_data_path\", type=str,\n",
    "                    help=\"Path to raw item document data. item document consists of multiple text. data format - item id::text1|text2...\")\n",
    "parser.add_argument(\"-m\", \"--min_rating\", type=int,\n",
    "                    help=\"Users who have less than \\\"min_rating\\\" ratings will be removed (default = 1)\", default=1)\n",
    "parser.add_argument(\"-l\", \"--max_length_document\", type=int,\n",
    "                    help=\"Maximum length of document of each item (default = 300)\", default=300)\n",
    "parser.add_argument(\"-f\", \"--max_df\", type=float,\n",
    "                    help=\"Threshold to ignore terms that have a document frequency higher than the given value (default = 0.5)\", default=0.5)\n",
    "parser.add_argument(\"-s\", \"--vocab_size\", type=int,\n",
    "                    help=\"Size of vocabulary (default = 8000)\", default=8000)\n",
    "parser.add_argument(\"-t\", \"--split_ratio\", type=float,\n",
    "                    help=\"Ratio: 1-ratio, ratio/2 and ratio/2 of the entire dataset (R) will be training, valid and test set, respectively (default = 0.2)\", default=0.2)\n",
    "\n",
    "# Option for pre-processing data and running ConvMF\n",
    "parser.add_argument(\"-d\", \"--data_path\", type=str,\n",
    "                    help=\"Path to training, valid and test data sets\")\n",
    "parser.add_argument(\"-a\", \"--aux_path\", type=str, help=\"Path to R, D_all sets\")\n",
    "\n",
    "# Option for running ConvMF\n",
    "parser.add_argument(\"-o\", \"--res_dir\", type=str,\n",
    "                    help=\"Path to ConvMF's result\")\n",
    "parser.add_argument(\"-e\", \"--emb_dim\", type=int,\n",
    "                    help=\"Size of latent dimension for word vectors (default: 200)\", default=200)\n",
    "parser.add_argument(\"-p\", \"--pretrain_w2v\", type=str,\n",
    "                    help=\"Path to pretrain word embedding model  to initialize word vectors\")\n",
    "parser.add_argument(\"-g\", \"--give_item_weight\", type=bool,\n",
    "                    help=\"True or False to give item weight of ConvMF (default = False)\", default=True)\n",
    "parser.add_argument(\"-k\", \"--dimension\", type=int,\n",
    "                    help=\"Size of latent dimension for users and items (default: 50)\", default=50)\n",
    "parser.add_argument(\"-u\", \"--lambda_u\", type=float,\n",
    "                    help=\"Value of user regularizer\")\n",
    "parser.add_argument(\"-v\", \"--lambda_v\", type=float,\n",
    "                    help=\"Value of item regularizer\")\n",
    "parser.add_argument(\"-n\", \"--max_iter\", type=int,\n",
    "                    help=\"Value of max iteration (default: 200)\", default=200)\n",
    "parser.add_argument(\"-w\", \"--num_kernel_per_ws\", type=int,\n",
    "                    help=\"Number of kernels per window size for CNN module (default: 100)\", default=100)\n",
    "\n",
    "args = parser.parse_args()\n",
    "do_preprocess = args.do_preprocess\n",
    "data_path = args.data_path\n",
    "aux_path = args.aux_path\n",
    "if data_path is None:\n",
    "    sys.exit(\"Argument missing - data_path is required\")\n",
    "if aux_path is None:\n",
    "    sys.exit(\"Argument missing - aux_path is required\")\n",
    "\n",
    "data_factory = Data_Factory()\n",
    "\n",
    "if do_preprocess:\n",
    "    path_rating = args.raw_rating_data_path\n",
    "    path_itemtext = args.raw_item_document_data_path\n",
    "    min_rating = args.min_rating\n",
    "    max_length = args.max_length_document\n",
    "    max_df = args.max_df\n",
    "    vocab_size = args.vocab_size\n",
    "    split_ratio = args.split_ratio\n",
    "\n",
    "    print \"=================================Preprocess Option Setting=================================\"\n",
    "    print \"\\tsaving preprocessed aux path - %s\" % aux_path\n",
    "    print \"\\tsaving preprocessed data path - %s\" % data_path\n",
    "    print \"\\trating data path - %s\" % path_rating\n",
    "    print \"\\tdocument data path - %s\" % path_itemtext\n",
    "    print \"\\tmin_rating: %d\\n\\tmax_length_document: %d\\n\\tmax_df: %.1f\\n\\tvocab_size: %d\\n\\tsplit_ratio: %.1f\" \\\n",
    "        % (min_rating, max_length, max_df, vocab_size, split_ratio)\n",
    "    print \"===========================================================================================\"\n",
    "\n",
    "    R, D_all = data_factory.preprocess(path_rating, path_itemtext, min_rating, max_length, max_df, vocab_size)\n",
    "    data_factory.save(aux_path, R, D_all)\n",
    "    data_factory.generate_train_valid_test_file_from_R(data_path, R, split_ratio)\n",
    "else:\n",
    "    res_dir = args.res_dir\n",
    "    emb_dim = args.emb_dim\n",
    "    pretrain_w2v = args.pretrain_w2v\n",
    "    dimension = args.dimension\n",
    "    lambda_u = args.lambda_u\n",
    "    lambda_v = args.lambda_v\n",
    "    max_iter = args.max_iter\n",
    "    num_kernel_per_ws = args.num_kernel_per_ws\n",
    "    give_item_weight = args.give_item_weight\n",
    "\n",
    "    if res_dir is None:\n",
    "        sys.exit(\"Argument missing - res_dir is required\")\n",
    "    if lambda_u is None:\n",
    "        sys.exit(\"Argument missing - lambda_u is required\")\n",
    "    if lambda_v is None:\n",
    "        sys.exit(\"Argument missing - lambda_v is required\")\n",
    "\n",
    "    print \"===================================ConvMF Option Setting===================================\"\n",
    "    print \"\\taux path - %s\" % aux_path\n",
    "    print \"\\tdata path - %s\" % data_path\n",
    "    print \"\\tresult path - %s\" % res_dir\n",
    "    print \"\\tpretrained w2v data path - %s\" % pretrain_w2v\n",
    "    print \"\\tdimension: %d\\n\\tlambda_u: %.4f\\n\\tlambda_v: %.4f\\n\\tmax_iter: %d\\n\\tnum_kernel_per_ws: %d\" \\\n",
    "        % (dimension, lambda_u, lambda_v, max_iter, num_kernel_per_ws)\n",
    "    print \"===========================================================================================\"\n",
    "\n",
    "    R, D_all = data_factory.load(aux_path)\n",
    "    CNN_X = D_all['X_sequence']\n",
    "    vocab_size = len(D_all['X_vocab']) + 1\n",
    "\n",
    "    from models import ConvMF\n",
    "\n",
    "    if pretrain_w2v is None:\n",
    "        init_W = None\n",
    "    else:\n",
    "        init_W = data_factory.read_pretrained_word2vec(pretrain_w2v, D_all['X_vocab'], emb_dim)\n",
    "\n",
    "    train_user = data_factory.read_rating(data_path + '/train_user.dat')\n",
    "    train_item = data_factory.read_rating(data_path + '/train_item.dat')\n",
    "    valid_user = data_factory.read_rating(data_path + '/valid_user.dat')\n",
    "    test_user = data_factory.read_rating(data_path + '/test_user.dat')\n",
    "\n",
    "    ConvMF(max_iter=max_iter, res_dir=res_dir, lambda_u=lambda_u, lambda_v=lambda_v,\n",
    "           dimension=dimension, vocab_size=vocab_size, init_W=init_W, give_item_weight=give_item_weight, \n",
    "           CNN_X=CNN_X, emb_dim=emb_dim, num_kernel_per_ws=num_kernel_per_ws,\n",
    "           train_user=train_user, train_item=train_item, valid_user=valid_user, test_user=test_user, R=R)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
